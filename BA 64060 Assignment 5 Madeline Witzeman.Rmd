---
title: "BA 64060 Assignment 5"
author: "Madeline Witzeman"
date: "2023-11-24"
output: html_document
---
# Load data into R

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
cereal.ds <- read.csv("C:\\Users\\Madeline\\Documents\\Kent State University\\Fall 2023 Semester\\BA 64060\\Cereals.csv")
```

# Load any potential required packages (after they've already been installed)

```{r}
library(tidyverse) 
library(factoextra)
library(cluster)
library(dplyr)
library(caret)
library(ISLR)
library(class)
```

# Data Preprocessing: remove the cereals with missing values; set cereal names as row names. For the purposes of this clustering analysis, I also removed the non-numerical variables.

```{r}
cereal.ds2 <- na.omit(cereal.ds)
row.names(cereal.ds2) <- cereal.ds2[,1]
cereal.ds2 <- cereal.ds2[,c(4,5,6,7,8,9,10,11,12,13,14,15,16)]
head(cereal.ds2)
```

# Step 1: Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward. Choose the best method.

# Normalize the data:

```{r}
norm.cereal.ds2 <- scale(cereal.ds2)
head(cereal.ds2)
```

# Use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward; choose the best method

```{r}
hc_single <- agnes(norm.cereal.ds2, method = "single")
hc_complete <- agnes(norm.cereal.ds2, method = "complete")
hc_average <- agnes(norm.cereal.ds2, method = "average")
hc_Ward <- agnes(norm.cereal.ds2, method = "ward")
print(hc_single$ac)
print(hc_complete$ac)
print(hc_average$ac)
print(hc_Ward$ac)
```
# Based on the agglomerative coefficients above, the Ward's method is the best

# Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements and the Ward's method for clustering

```{r}
d <- dist(norm.cereal.ds2, method = "euclidean")
hc1 <- hclust(d, method = "ward.D" )
plot(hc1, cex = 0.6, hang = -1)
```
# Step 2: How many clusters would you choose?

# I would choose 4 clusters based on the hierarchical clustering output above:

```{r}
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 4, border = 2:5)
```
```{r}
memb1 <- cutree(hc1, k = 4)
memb1
```



# Step 3: Comment on the structure of the clusters and on their stability

# The clusters vary in size from roughly 12 to 25 data points. The red cluster (Cluster 3) seems to be characterized by below average fiber & potassium and above average sugar. The green cluster (Cluster 2) is characterized by above average calories, fat, fiber, potassium and average to above average sugar. The dark blue cluster (Cluster 4) is characterized by average to below average fat, below average sugar, and above average sodium & carbohydrates. Lastly, the aqua cluster (Cluster 1) is characterized by below average calories, fat, sodium, & sugar, mostly above average protein, fiber, potassium, & rating, and average vitamins.

# Testing the stability:

# Partitioning the data (70% Partition A, 30% Partition B)

```{r}
set.seed(18)
names(norm.cereal.ds2) <- c('calories', 'protein', 'fat', 'sodium', 'fiber', 'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups', 'rating')
df.norm.cereal.ds2<- as.data.frame(norm.cereal.ds2)
  
PartA_Index_Cereal = createDataPartition(df.norm.cereal.ds2$calories, p=0.7, list=FALSE)
PartA_Data_Cereal = df.norm.cereal.ds2[PartA_Index_Cereal,]
PartB_Data_Cereal = df.norm.cereal.ds2[-PartA_Index_Cereal,]
```

# Cluster training data (Partition A)

```{r}
d2 <- dist(PartA_Data_Cereal, method = "euclidean")
hc2 <- hclust(d2, method = "ward.D" )
plot(hc2, cex = 0.6, hang = -1)
rect.hclust(hc2, k = 4, border = 2:5)
```
```{r}
memb2 <- cutree(hc2, k = 4)
memb2
```


# Use the cluster centroids from Partition A to assign each record in Partition B (each record is assigned to the cluster with the closest centroid).

# Compute the mean/average for each numerical variable in each cluster in partition A (centroids for Partition A).

```{r}
membA <- cutree(hc2, k = 4)
membA
PartA_Centroids <- aggregate(PartA_Data_Cereal,list(cluster=membA),mean)
PartA_Centroids
```


# Find which cluster in partition A has the closest/smallest Euclidean distance to each observation in partition B.

```{r}
PartA_Centroids_adj <- PartA_Centroids[,2:14]
PartB_Data_Cereal_df <- as.data.frame(PartB_Data_Cereal)
PartACentPartB <- bind_rows(PartA_Centroids_adj, PartB_Data_Cereal_df) 
PartACentPartB
```

```{r}
distAB <- get_dist(PartACentPartB, method = "euclidean")
print(distAB)
```
# Based on the euclidean distances between the cluster centroids in Partition A and the cereals in Partition B above, the cereals in Partition B would be clustered as the following:
# All-Bran: Cluster 1 (same as original cluster)                             
# All-Bran_with_Extra_Fiber: Cluster 1 (same as orig cluster)       
# Basic_4: Cluster 2 (same as orig cluster)                           
# Cheerios: Cluster 4 (same as orig cluster)                        
# Corn_Chex: Cluster 4 (same as orig cluster)                         
# Corn_Pops: Cluster 3 (same as orig cluster)                       
# Crispy_Wheat_&_Raisins: Cluster 3 (different - orig cluster is 2)           
# Froot_Loops: Cluster 3 (same as orig cluster)                       
# Fruitful_Bran: Cluster 2 (same as orig cluster)                    
# Fruity_Pebbles: Cluster 3 (same as orig cluster)                    
# Honey_Nut_Cheerios: Cluster 3 (same as orig cluster)                
# Just_Right_Fruit_&_Nut: Cluster 2 (different - orig cluster is 4)           
# Kix: Cluster 4 (same as orig cluster)                               
# Muesli_Raisins,_Peaches,_&_Pecans: Cluster 2 (same as orig cluster)  
# Multi-Grain_Cheerios: Cluster 3 (same as orig cluster)              
# Nutri-grain_Wheat: Cluster 1 (different - orig cluster is 4)       
# Quaker_Oat_Squares: Cluster 1 (different - orig cluster is 2)                
# Raisin_Bran: Cluster 2 (same as orig cluster)                       
# Special_K: Cluster 1 (different - orig cluster is 4)                         
# Total_Whole_Grain: Cluster 4 (same as orig cluster)                 
# Wheaties: Cluster 1 (different - orig cluster is 4)   

# When comparing these clusters to the orginal clusters produced based on the whole dataset (during steps 1/2), the clusters seem to be somewhat unstable. Clustering based on different partitions of the data resulted in 6 cereals (28.57% of Partition B) changing clusters.

# (I couldn't determine how to use a 'for loop' to solve this problem so I approached it differently)


# Step 4: Find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?

# Yes, the data should be normalized. The distance metrics utilized in hierarchical clustering are highly sensitive to the scale of variables. If not normalized, variables with a larger scale will have a stronger effect on total distance. I've already normalized the data during my clustering analysis above. 

# I would select the aqua cluster (Cluster 1) as the healthiest cereal category. The cereals in this cluster are predominantly low calories, fat, sodium, & sugar while being higher protein, potassium, & fiber, with average vitamins.
